{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{General questions:}$\n",
    "\n",
    "$\\textbf{1. How accurate is the metod, depending on data distribution?}$\n",
    "\n",
    "$\\textbf{2. How efficient is the metod?}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Collaborative filtering}$\n",
    "\n",
    "This is the first of our approaches, probably most classical one. The idea behind it is quite simple: to estimate user's grade, we can look at already given grades from some other users with similar preferences. It requires also just some generally easily accesible data: knowledge about previous scores of each user.\n",
    "\n",
    "Here come some important questions:\n",
    "\n",
    "$\\textbf{1. How to define two users similarity?}$\n",
    "\n",
    "$\\textbf{2. How many most simliar users is optimal to choose?}$\n",
    "\n",
    "$\\textbf{3. How to exactly estimate our score, after we chosed our nearest neighbours?}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the first question. To answer it, we should choose some function, which defines distances between every pair of users. We can use such distances as similarity measure.\n",
    "\n",
    "How such function such behave? Obviously, the most desirable is the following general property: less distance means more similar preferences. It should be also correctly defined for each pair of possibly compared pair of users, to avoid some technical problems (like dividing by zero). We also wanted to choose something which is quite fast to compute. It actually deosn't have to be metric, which comes to our minds at first - it is enough if it allows to choose some $k$ most similar users to the given one.\n",
    "\n",
    "After braistorms and research, we decided to compare the following approaches. Each of them focuses on the subset of animes watched by both chosen users. User's grades assigned to them form in natural way a vector and for such pair of vectors we can compute:\n",
    "\n",
    "$\\textbf{1. Reciprocal of number of elements}$\n",
    "\n",
    "This is our first idea. It is really simple and fast to come with and compute. Our intuituion was that preferences may be guessed looking at commonly watched animes. However, such approach ignores actual grades and also seems to be easly biased by popular series.\n",
    "\n",
    "$\\textbf{2. Sum of squares of differences between elements, divided by number of elements}$\n",
    "\n",
    "This is improvement of first idea: we can include the information about absolute differences between grades and use it as the main factor of deciding about similar preferences. To avoid favourizing short vectors (which, we balieve, tend more to be randomly biased), we remain division by number of elements.\n",
    "\n",
    "$\\textbf{3. Pearson correlation coefficient}$\n",
    "\n",
    "Approuch found in some other researchers' papers.\n",
    "\n",
    "$\\textbf{4. Cosine}$\n",
    "\n",
    "As above, second well-known function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second question is also not so obvious to answer. Let's call our wanted parameter $k$. General intuition tells us, that function of error for growing $k$ should be bitonic: at the beginning, error is decreasing, since we are gaining more and more information. However, from some point our information are more and more biased, since we include more and more not so similar users - it leads to increasing error.\n",
    "\n",
    "So we can think about some ternary search on something like that, but simpler and more cetrain method is to try each $k$ less than some bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads us to the third question. Knowing which $k$ users are most similar to us, we want to make our final guess. At the beginning on this part, we assumed that we only focus on how users scored some subsets of animes. In particular, we don't want to think now about some direct information about animes themselves (like if they cover similar topics). So, the best option seems to take average score of our desired anime among chosen users.\n",
    "\n",
    "There are some problems with this approach, however they can be easily solved:\n",
    "\n",
    "$\\textbf{1. It may happen that most of our nearest neighbours haven't watched our anime yet.}$\n",
    "\n",
    "To solve that issue, after selecting which anime we want to process, we temporary erase from dataset people who haven't watched it yet.\n",
    "\n",
    "$\\textbf{2. The estimation is not an integer}$\n",
    "\n",
    "In practice it shouldn't be a problem, since for our system it seems enough to just give some approximation - we don't have to simulate exact score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
